{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a46ff8-1aeb-40a9-ab2e-53dbddabbf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import git\n",
    "import pathlib\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # use GPU\n",
    "# Using GPU during inference has deterministic results (same as CPU)\n",
    "\n",
    "PROJ_ROOT_PATH = pathlib.Path(git.Repo('.', search_parent_directories=True).working_tree_dir)\n",
    "PROJ_ROOT =  str(PROJ_ROOT_PATH)\n",
    "if PROJ_ROOT not in sys.path:\n",
    "    sys.path.append(PROJ_ROOT)\n",
    "\n",
    "from libs import utils, mnist32_cnn\n",
    "from libs.constants import model_seeds\n",
    "from libs.errmatmul import matmul_ERRexpbitflips, N_THREADS_PER_BLOCK, NO_OF_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24272124-9be7-4c55-ba5a-a6dcbb471c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit GPU growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f854f283-e420-4f70-a699-b3b84b69d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "# Combine test and train images together into one dataset\n",
    "DATASET_PATH = str(pathlib.Path(PROJ_ROOT_PATH / \"datasets\" / \"mnist.npz\" ))\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data(path=DATASET_PATH)\n",
    "train_images = train_images.astype(np.float32) / 255.0\n",
    "test_images = test_images.astype(np.float32) / 255.0  \n",
    "\n",
    "all_images =np.concatenate([train_images, test_images], axis=0)\n",
    "all_labels =np.concatenate([train_labels, test_labels], axis=0)\n",
    "all_images = np.expand_dims(all_images, axis=-1)\n",
    "\n",
    "# resize the input shape , i.e. old shape: 28, new shape: 32\n",
    "image_x_size = 32\n",
    "image_y_size = 32\n",
    "all_images = tf.image.resize(all_images, [image_x_size, image_y_size]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65d1c4c-9cb3-4c6f-9b42-606a0398d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"mnist32-cnn_1024_256_64\"\n",
    "model_seed = model_seeds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e34c2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "model_instance = model_type + \"-\" + str(model_seed)\n",
    "dataset, model_arch, model_config, layer_widths, seed = utils.instancename2metadata(model_instance)\n",
    "model_meta_type, model_type, model_instance = utils.metadata2instancenames(dataset, model_arch, layer_widths, seed)\n",
    "\n",
    "model_folder = pathlib.Path(PROJ_ROOT_PATH / \"models\" / model_meta_type / model_type)\n",
    "model_filename = model_instance + \".h5\"\n",
    "model_file = pathlib.Path(model_folder/ model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bde2915-dff1-43e8-97c7-04880fa86289",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_im = 32 # no of images per batch\n",
    "images = all_images[0:no_im]\n",
    "# image = tf.expand_dims(all_images[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b99335f-83ef-41bc-985a-c30d7fd6bade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 32, 32, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90befff6-7fb9-4713-8ae2-6f2355bd42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_error_profile = np.zeros((20_000,32), dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad148d51-7fe2-44ce-9c70-efa8eb5102e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 29, 29, 32)        544       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " fc_0 (Dense)                (None, 1024)              6423552   \n",
      "                                                                 \n",
      " fc_1 (Dense)                (None, 256)               262400    \n",
      "                                                                 \n",
      " fc_2 (Dense)                (None, 64)                16448     \n",
      "                                                                 \n",
      " op_layer (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,703,594\n",
      "Trainable params: 6,703,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = tf.keras.models.load_model(model_file)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa42fef3-d03b-48f7-ada9-0f75d181744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights and biases from model\n",
    "conv2d_kernels, conv2d_biases = model.get_layer(\"conv2d\").weights\n",
    "fc_0_weights, fc_0_biases = model.get_layer(\"fc_0\").weights\n",
    "fc_1_weights, fc_1_biases = model.get_layer(\"fc_1\").weights\n",
    "fc_2_weights, fc_2_biases = model.get_layer(\"fc_2\").weights\n",
    "op_layer_weights, op_layer_biases = model.get_layer(\"op_layer\").weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70b8f092-96c5-44b9-9e55-279f07d77929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv2d_kernels.shape, conv2d_biases.shape\n",
    "# fc_0_weights.shape, fc_0_biases.shape\n",
    "# fc_1_weights.shape, fc_1_biases.shape\n",
    "# fc_2_weights.shape, fc_2_biases.shape\n",
    "# op_layer_weights.shape, op_layer_biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "722755ac-3fab-4baa-a786-01b7e83144e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution layer\n",
    "## get dimension constants\n",
    "kr_ht, kr_wt, no_ch, no_kr = conv2d_kernels.shape\n",
    "\n",
    "assert no_im == images.shape[0]\n",
    "im_ht = images.shape[1]\n",
    "im_wt = images.shape[2]\n",
    "assert no_ch == images.shape[-1]\n",
    "\n",
    "y_ht = im_ht - kr_ht + 1\n",
    "y_wt = im_wt - kr_wt + 1\n",
    "\n",
    "no_of_patches = y_ht * y_wt\n",
    "patch_len     = kr_ht * kr_wt * no_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "131395c5-05a9-4080-9324-c49666d04b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract images patches\n",
    "patches = tf.image.extract_patches(images=images,\n",
    "                                 sizes=[1, kr_ht, kr_wt, 1],\n",
    "                                 strides=[1, 1, 1, 1],\n",
    "                                 rates=[1, 1, 1, 1],\n",
    "                                 padding='VALID')\n",
    "## flatten patches\n",
    "flat_patches = tf.reshape(patches, (no_im, no_of_patches, patch_len))\n",
    "## tranpose for matrix multiplication\n",
    "flat_patches = tf.transpose(flat_patches, (0,2,1))\n",
    "\n",
    "## flatten filter kernels\n",
    "### first reorder kernels by no. of output-kernels\n",
    "flat_kernels = tf.transpose(conv2d_kernels, perm=(3,0,1,2))\n",
    "flat_kernels = tf.reshape(flat_kernels, (no_kr, kr_ht*kr_wt*no_ch))\n",
    "# flat_kernels = tf.broadcast_to(flat_kernels, (no_im, no_kr, kr_ht*kr_wt*no_ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b226d73-1ccc-469e-bdbe-675b64b29c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 16])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_kernels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ecdc0ba-9139-4240-9483-0584ae47d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "## perform matrix multiplication\n",
    "# conv_out = tf.matmul(flat_kernels, flat_patches)\n",
    "conv_out_list = []\n",
    "for im in range(no_im):\n",
    "    single_im_patch = flat_patches[im,:,:]\n",
    "    # conv_out_list.append(tf.matmul(flat_kernels, single_im_patch))\n",
    "    BLOCK_HEIGHT = N_THREADS_PER_BLOCK # no. of threads per block\n",
    "    BLOCK_WIDTH = kr_ht*kr_wt # totcols is always (going to be) a multiple of BLOCK_WIDTH\n",
    "    BATCH_BLOCK_SIZE = 32 # user-defined: NOT the actual batch block size in this context.\n",
    "                            # simply the tile block width of matB\n",
    "    # pad matrix for good matrix shape\n",
    "    no_cols_to_pad = BATCH_BLOCK_SIZE-(single_im_patch.shape[1]%BATCH_BLOCK_SIZE)\n",
    "    paddings = tf.constant([[0, 0,], # padding above and below\n",
    "                            [0, no_cols_to_pad]]) # padding left and right\n",
    "    padded_single_im_patch = tf.pad(single_im_patch, \n",
    "                                    paddings,\n",
    "                                    mode=\"CONSTANT\", \n",
    "                                    constant_values=0.0)\n",
    "    conv_out_list.append(matmul_ERRexpbitflips(flat_kernels, \n",
    "                                               padded_single_im_patch,\n",
    "                                               BLOCK_HEIGHT, \n",
    "                                               BLOCK_WIDTH, \n",
    "                                               BATCH_BLOCK_SIZE, \n",
    "                                               ERR_PROFILE=dummy_error_profile,\n",
    "                                               ERR_PARAM_TF=None,)[:,:-no_cols_to_pad])\n",
    "conv_out = tf.stack(conv_out_list)\n",
    "conv_out = tf.transpose(conv_out, (0,2,1))\n",
    "conv_out = tf.reshape(conv_out, (no_im, y_ht,y_wt, no_kr))\n",
    "\n",
    "## Add bias\n",
    "conv_out = tf.nn.bias_add(conv_out, conv2d_biases)\n",
    "## ReLU\n",
    "conv_out = tf.nn.relu(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d2494f4-7a7a-4c51-b743-74cb45b193f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Sanity Check AGAIN\n",
    "# tf.reduce_sum(conv_out - model.layers[0](images))\n",
    "\n",
    "# Sanity check for convolutional layer\n",
    "# tf_conv_out = tf.nn.conv2d(input=images,\n",
    "#                             filters=conv2d_kernels,\n",
    "#                             strides=[1,1,1,1],\n",
    "#                             padding=\"VALID\",\n",
    "#                             data_format='NHWC',\n",
    "#                             dilations=None,\n",
    "#                             name=None\n",
    "#                         )\n",
    "\n",
    "# # convolutional layer sanity check\n",
    "# tf_conv_layer_dummy = tf.keras.layers.Conv2D(32, (4, 4), \n",
    "#                                   activation='relu', \n",
    "#                                   input_shape=(32, 32, 1))\n",
    "# # get layer running\n",
    "# dummy_input = tf.random.normal((1,32,32,1))\n",
    "# tf_conv_layer_dummy(dummy_input);\n",
    "\n",
    "# # load the weights\n",
    "# tf_conv_layer_dummy.set_weights([conv2d_kernels, conv2d_biases])\n",
    "# # run the convolution, bias adding and relu\n",
    "# tf_conv_layer_out = tf_conv_layer_dummy(images);\n",
    "# # check if outputs match\n",
    "# tf.reduce_sum(conv_out-tf_conv_layer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "444dce28-6101-468a-bc4c-2135d9817afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Max Pooling\n",
    "pool_out = tf.nn.max_pool(\n",
    "                        conv_out,\n",
    "                        ksize=[1, 2, 2, 1], #(batch_size, height, width, depth)\n",
    "                        strides=[1, 2, 2, 1], #(batch_size, height, width, depth)\n",
    "                        padding='VALID')\n",
    "# pool_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44d6e278-aecd-468c-bc03-68321df05897",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Sanity Check for pooling layer\n",
    "# tf.reduce_sum(pool_out - model.layers[1](model.layers[0](images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98bb6b7f-a753-4590-b401-81336cb41abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten\n",
    "flat_out = tf.reshape(pool_out, (no_im, -1) ) #[batch_size, flat_vec_size]\n",
    "# flat_out.shape\n",
    "# # Tranpose for multiplication\n",
    "# flat_out = tf.transpose(flat_out, perm=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e39360f-99c3-4837-8d1f-89f7eb16da5a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Sanity Check for flatten layer\n",
    "# tf.reduce_sum(flat_out - model.layers[2](model.layers[1](model.layers[0](images))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f187af75-b953-4bee-bba3-a0e347933761",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Sanity Check for flatten layer\n",
    "# flayer = tf.keras.layers.Flatten()\n",
    "# flayer(pool_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "805750bf-7314-4408-99b8-0fb84f89c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranpose input vector\n",
    "fc_0_in = tf.transpose(flat_out, perm=[1,0]) #[flat_vec_size, batch_size]\n",
    "# fc_0_in.shape\n",
    "\n",
    "# transpose weight matrices\n",
    "fc_0_weights_tr = tf.transpose(fc_0_weights, perm=[1,0]) #[no_of_weights, flat_vec_size]\n",
    "# fc_0_weights_tr.shape\n",
    "\n",
    "# Multiply input with weights\n",
    "# fc_0_mult_out = tf.linalg.matmul(fc_0_weights_tr, fc_0_in)\n",
    "BLOCK_HEIGHT = N_THREADS_PER_BLOCK # no. of threads per block\n",
    "BLOCK_WIDTH = 32 # totcols is always (going to be) a multiple of BLOCK_WIDTH\n",
    "BATCH_BLOCK_SIZE = 1 # user-defined: assuming batchsize is always a multiple of BATCH_BLOCK_SIZE\n",
    "fc_0_mult_out = matmul_ERRexpbitflips(fc_0_weights_tr, \n",
    "                                       fc_0_in,\n",
    "                                       BLOCK_HEIGHT, \n",
    "                                       BLOCK_WIDTH, \n",
    "                                       BATCH_BLOCK_SIZE, \n",
    "                                       ERR_PROFILE=dummy_error_profile,\n",
    "                                       ERR_PARAM_TF=None,)\n",
    "\n",
    "# Add bias\n",
    "fc_0_bout = tf.add(fc_0_mult_out, tf.expand_dims(fc_0_biases,axis=1))\n",
    "# RelU\n",
    "fc_0_out = tf.nn.relu(fc_0_bout)\n",
    "# fc_0_out needs to be transposed again in fc_1_in\n",
    "# so although fc_0_out shape is not \"standard\", we output it as it is\n",
    "# fc_0_out = tf.transpose(fc_0_out, perm=[1,0]) #[batch_no, vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fea72ccd-97c2-43a7-b491-29359495793e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Sanity Check for fc_0_out\n",
    "# tf.reduce_max( tf.abs(tf.transpose(fc_0_out, perm=[1,0]) - model.layers[3](model.layers[2](model.layers[1](model.layers[0](images))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0058de2-33be-4f77-bce8-b6aa8b764372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't tranpose fc_0_out to convert to fc_1_in\n",
    "# fc_1_in = tf.transpose(flat_out, perm=[1,0])\n",
    "fc_1_in = fc_0_out\n",
    "fc_1_weights_tr = tf.transpose(fc_1_weights, perm=[1,0])\n",
    "\n",
    "# fc_1_mult_out = tf.linalg.matmul(fc_1_weights_tr, fc_1_in)\n",
    "BLOCK_HEIGHT = N_THREADS_PER_BLOCK # no. of threads per block\n",
    "BLOCK_WIDTH = 32 # totcols is always (going to be) a multiple of BLOCK_WIDTH\n",
    "BATCH_BLOCK_SIZE = 1 # user-defined: assuming batchsize is always a multiple of BATCH_BLOCK_SIZE\n",
    "fc_1_mult_out = matmul_ERRexpbitflips(fc_1_weights_tr, \n",
    "                                       fc_1_in,\n",
    "                                       BLOCK_HEIGHT, \n",
    "                                       BLOCK_WIDTH, \n",
    "                                       BATCH_BLOCK_SIZE, \n",
    "                                       ERR_PROFILE=None,\n",
    "                                       ERR_PARAM_TF=None,)\n",
    "\n",
    "fc_1_bout = tf.add(fc_1_mult_out, tf.expand_dims(fc_1_biases,axis=1))\n",
    "fc_1_out = tf.nn.relu(fc_1_bout)\n",
    "# fc_1_out needs to be transposed again in fc_2_in\n",
    "# so although fc_1_out shape is not \"standard\", we output it as it is\n",
    "# fc_1_out = tf.transpose(fc_1_out, perm=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14261070-6cc5-4be5-b6ed-0b6b6e42ffca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Sanity Check for fc_1_out\n",
    "# tf.reduce_max( tf.abs(tf.transpose(fc_1_out, perm=[1,0]) - \n",
    "#                       model.layers[4](\n",
    "#                           model.layers[3](\n",
    "#                               model.layers[2](\n",
    "#                                   model.layers[1](\n",
    "#                                       model.layers[0](images)))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ed7e5b4-c365-429c-8e93-7a5594d03936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't tranpose fc_1_out to convert to fc_2_in\n",
    "# fc_2_in = tf.transpose(fc_1_out, perm=[1,0])\n",
    "fc_2_in = fc_1_out\n",
    "fc_2_weights_tr = tf.transpose(fc_2_weights, perm=[1,0])\n",
    "\n",
    "# fc_2_mult_out = tf.linalg.matmul(fc_2_weights_tr, fc_2_in)\n",
    "BLOCK_HEIGHT = N_THREADS_PER_BLOCK # no. of threads per block\n",
    "BLOCK_WIDTH = 32 # totcols is always (going to be) a multiple of BLOCK_WIDTH\n",
    "BATCH_BLOCK_SIZE = 1 # user-defined: assuming batchsize is always a multiple of BATCH_BLOCK_SIZE\n",
    "fc_2_mult_out = matmul_ERRexpbitflips(fc_2_weights_tr, \n",
    "                                       fc_2_in,\n",
    "                                       BLOCK_HEIGHT, \n",
    "                                       BLOCK_WIDTH, \n",
    "                                       BATCH_BLOCK_SIZE, \n",
    "                                       ERR_PROFILE=None,\n",
    "                                       ERR_PARAM_TF=None,)\n",
    "\n",
    "fc_2_bout = tf.add(fc_2_mult_out, tf.expand_dims(fc_2_biases,axis=1))\n",
    "fc_2_out = tf.nn.relu(fc_2_bout)\n",
    "# fc_2_out needs to be transposed again in op_layer_in\n",
    "# so although fc_2_out shape is not \"standard\", we output it as it is\n",
    "# fc_2_out = tf.transpose(fc_2_out, perm=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "327fe847-79b9-46a4-884a-6545e9095d16",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Sanity Check for fc_2_out\n",
    "# tf.reduce_max( tf.abs(tf.transpose(fc_2_out, perm=[1,0]) - \n",
    "#                       model.layers[5](\n",
    "#                           model.layers[4](\n",
    "#                               model.layers[3](\n",
    "#                                   model.layers[2](\n",
    "#                                       model.layers[1](\n",
    "#                                           model.layers[0](images))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "956c1bd0-cba8-4596-836d-7b0938dff76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 36 calls to <function matmul_ERRexpbitflips at 0x7fce7ce2e790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# We don't tranpose fc_2_out to convert to op_layer_in\n",
    "# fc_2_in = tf.transpose(fc_1_out, perm=[1,0])\n",
    "op_layer_in = fc_2_out\n",
    "op_layer_weights_tr = tf.transpose(op_layer_weights, perm=[1,0])\n",
    "\n",
    "# op_layer_mult_out = tf.linalg.matmul(op_layer_weights_tr, op_layer_in)\n",
    "BLOCK_HEIGHT = NO_OF_CLASSES # no. of threads per block\n",
    "BLOCK_WIDTH = 32 # totcols is always (going to be) a multiple of BLOCK_WIDTH\n",
    "BATCH_BLOCK_SIZE = 1 # user-defined: assuming batchsize is always a multiple of BATCH_BLOCK_SIZE\n",
    "op_layer_mult_out = matmul_ERRexpbitflips(op_layer_weights_tr, \n",
    "                                       op_layer_in,\n",
    "                                       BLOCK_HEIGHT, \n",
    "                                       BLOCK_WIDTH, \n",
    "                                       BATCH_BLOCK_SIZE, \n",
    "                                       ERR_PROFILE=None,\n",
    "                                       ERR_PARAM_TF=None,)\n",
    "\n",
    "op_layer_bout = tf.add(op_layer_mult_out, tf.expand_dims(op_layer_biases,axis=1))\n",
    "op_layer_out = tf.nn.softmax(op_layer_bout, axis=0)\n",
    "op_layer_out = tf.transpose(op_layer_out, perm=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a0eccc4-ae2a-4576-aa4c-6cde081fa9f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Sanity Check for op_layer_out\n",
    "# tf.reduce_max( tf.abs(op_layer_out - \n",
    "#                       model.layers[6](\n",
    "#                           model.layers[5](\n",
    "#                               model.layers[4](\n",
    "#                                   model.layers[3](\n",
    "#                                       model.layers[2](\n",
    "#                                           model.layers[1](\n",
    "#                                               model.layers[0](images)))))))))\n",
    "\n",
    "# Sanity Check 2 for op_layer_out\n",
    "# tf.reduce_max(op_layer_out - model(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0f3089b-fd52-4b6a-97f6-5ee8a8a0aa0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0,\n",
       "       9, 1, 1, 2, 4, 3, 2, 7, 3, 8])>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_a = tf.math.argmax(op_layer_out, axis=1)\n",
    "predictions_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2866bd13-d7cf-4a68-acc9-b34e9a70e594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0,\n",
       "       9, 1, 1, 2, 4, 3, 2, 7, 3, 8])>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_b = tf.math.argmax(model(images), axis=1)\n",
    "predictions_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc411a72-80f5-4a56-9f59-484edd830ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(predictions_a - predictions_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcf591bc-2e79-4d7a-8723-85e6fc045b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.8076192e-05>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(op_layer_out - model(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b88ba9ab-2967-4708-b94d-c586ea8011c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_fc_layers = 1 #initalized to 1 to account for output layer\n",
    "for layer in model.layers:\n",
    "    if layer.name.startswith(\"fc\"):\n",
    "        no_of_fc_layers += 1\n",
    "\n",
    "no_of_fc_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e53aa689-dd29-4c39-b76d-840f35a44c0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-34-bc44c73e846f>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [34]\u001b[0;36m\u001b[0m\n\u001b[0;31m    clayer0_shuffle_order,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# calculate_fitness(model)\n",
    "# eval_lenet_3hidden_errexpbitflip(model,\n",
    "#                                 error_profile_c0,\n",
    "#                                 error_profile_h0,\n",
    "#                                 error_profile_h1,\n",
    "#                                 error_profile_h2,\n",
    "#                                 error_profile_op,\n",
    "#                                 ERR_PARAM,\n",
    "                                clayer0_shuffle_order,\n",
    "                                hlayer0_shuffle_order,\n",
    "                                hlayer1_shuffle_order,\n",
    "                                hlayer2_shuffle_order,\n",
    "                                oplayer_shuffle_order,\n",
    "#                                 test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ac77f-60e2-4420-9a7e-dca245a0727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ff_dense_alllayer_2hidden_ERRexpbitflips(model, \n",
    "#                                         error_profile_c0,\n",
    "#                                         error_profile_h0,\n",
    "#                                         error_profile_h1,\n",
    "#                                         error_profile_h2,\n",
    "#                                         error_profile_op,\n",
    "#                                         ERR_PARAM,\n",
    "#                                         clayer0_shuffle_order,\n",
    "#                                         hlayer0_shuffle_order,\n",
    "#                                         hlayer1_shuffle_order,\n",
    "#                                         hlayer2_shuffle_order,\n",
    "#                                         oplayer_shuffle_order,\n",
    "#                                         test_set, \n",
    "#                                         batchsize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
